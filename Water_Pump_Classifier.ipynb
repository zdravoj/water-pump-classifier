{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanzania Water Pump Classifier\n",
    "\n",
    "The Tanzanian government has worked in conjunction with for-profit and non-profit organizations to build water pumps across Tanzania to provide its denizens with potable water. These pumps need maintenance to continue to operate. It is expensive, time-consuming, and inefficient to send repair teams, and tools and parts, only when the government receives reports of a faulty pump. The purpose of this project is to create a machine learning algorithm that can predict which pumps need repair or replacement to save the time and money of the Tanzanian government and its cooperative organizations.\n",
    "\n",
    "This data was from a private Kaggle competition held by BloomTech for its DS36 Data Science cohort; its data mirrors that of the community Kaggle competition.\n",
    "\n",
    "This documentation presents the project narrative in a CRISP-DM process style."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "All libraries used throughout the notebook will be initialized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "We want to create a machine learning algorithm that can predict which pumps are non-functional. Let's expand on this goal by further developing our understanding of the stakeholder's needs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "The Tanzanian government wants to keep the pumps in working order to provide a reliable source of water for its population. They have \"hired\" us to build a model that can turn accessible water pump data into three classes of predictions:\n",
    "\n",
    "1. Functional (the pump is working)\n",
    "2. Functional needs repair (the pump needs repair)\n",
    "3. Non functional (the pump needs replacement)\n",
    "\n",
    "With the predictions, the Tanzanian government will send the necessary tools and teams to the pumps needing repair, and replacement teams to the broken pumps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Factors and Considerations\n",
    "\n",
    "We should be prepared for the inevitability that our model is unlikely predict with perfect accuracy. An imperfect classification model is subject to Type I and Type II errors. In the case of water pump classification:\n",
    "\n",
    "- Type I, False Positive: The model predicts that the pump needs repair or replacement when it is in working condition\n",
    "- Type II, False Negative: The model predicts that the pump is in working condition when it needs repair or replacement\n",
    "\n",
    "Both types of errors should be avoided, but a Type II error is much more pressing. The worst case scenario in a Type II error would be when the model predicts a pump to be in working condition when it is broken and needs replacement. We're dealing with water, which is necessary to live. A Type I error would result in a waste of resources (sending repair teams and tools to a working pump), but a Type II error may result in a loss of lives!\n",
    "\n",
    "We could tune our model to eliminate Type II errors, but this would result in more Type I errors; more pumps that are functional would be marked as needing repair or replacement, which would be an unnecessary drain on repair resources.\n",
    "\n",
    "If we were able to meet with the stakeholder (the Tanzanian government) we would want to clarify how we should prioritize the reduction of these errors.\n",
    "\n",
    "We will assume that the stakeholder prefers to begin with a model that is as accurate as possible, and make adjustments to account for these errors when we have an MVP (minimum viable product)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metric\n",
    "\n",
    "For the MVP, we will focus on building a model that is as accurate as possible. There are several metrics we can use, which have their own benefits and drawbacks:\n",
    "\n",
    "1. Basic accuracy metric\n",
    "    - Pros: simple\n",
    "    - Cons: does not account for precision and recall (measures impact of Type I and Type II errors)\n",
    "2. Weighted F1 Score\n",
    "    - Pros: accounts for precision and recall\n",
    "    - Cons: does not directly account for poor scores in one classification (averages all classification scores)\n",
    "3. Multiclass ROC AUC\n",
    "    - Pros: accounts for precision and recall; accounts for poor scores in one classification; easiest to visualize; can average scores for one final metric\n",
    "    - Cons: introduces subjective assessment of success (multiple measures are weighed, assessor must rank measure importance)\n",
    "\n",
    "In accordance with the idea that the stakeholder may want to make adjustments to the model to account for errors, multiclass ROC AUC is the best choice. We will implement a 'One vs Rest' style of multiclass ROC AUC, which will give us a total of three graphs, with each graph comparing one feature to the other two. Changes to precision and recall for each class will be most visible while using this metric. In addition, if we wanted a single measure of accuracy, we retain the option to consolidate the scores for each graph into a simple or weighted average."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Plan\n",
    "\n",
    "Let's outline a plan for how we will approach this project.\n",
    "\n",
    "1. Data Understanding\n",
    "    - *Identify data surface qualities*: What is our data format? How many records do we have to train our model?\n",
    "    - *Verify data quality*: How dirty is the data? Are there any non-existent values? Incorrect values or types?\n",
    "    - *Understand features*: What kinds of data are available?\n",
    "    - *Explore data*: What relationships are present in the data? How do we visualize them?\n",
    "2. Data Preparation\n",
    "    - *Select / exclude data*: What features and records will we keep or remove? Why?\n",
    "    - *Clean data*: What values do we need to add, remove, or alter from when we verified data quality?\n",
    "    - *Format data*: Are the datatypes accurate? Can they be changed? Which are the most useful to our model?\n",
    "    - *Feature engineering*: Can we introduce better features, or consolidate the features we have to reduce dimensionality?\n",
    "3. Modeling\n",
    "    - *Select modeling techniques*: What models will we try?\n",
    "    - *Determine test design*: How will we test our model?\n",
    "    - *Build model*: How do we construct our model?\n",
    "    - *Tune model*: How do we change our model to be more accurate?\n",
    "    - *Assess model*: What are the results of our tests?\n",
    "4. Evaluation\n",
    "    - *Evaluate model result*: Does our model meet the stakeholder's requirements?\n",
    "    - *Review process*: Did we miss anything? What else should be added?\n",
    "    - *Determine next steps*: Should we deploy the model, or iterate further?\n",
    "5. Deployment\n",
    "    - We expect the stakeholder will want to make this model available for use beyond our project. We will need to create a way for users to provide data to our model, and retrieve its predictions.\n",
    "\n",
    "The plan is not intended to be strictly sequential - rather, it is intended to be flexible to allow us to return to previous sections to re-evaluate our approach when new information arises or project circumstances change."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Before we can manipulate our data or build a model from it, we must understand it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our data from the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into dataframe\n",
    "df = pd.merge(\n",
    "    pd.read_csv('train_labels.csv').set_index('id'),\n",
    "    pd.read_csv('train_features.csv').set_index('id'),\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "The Kaggle competition originally created for this project had its own private dataset. Since we do not have access to this dataset, we will need to create a testing set from the available data. We will need to know how many observations we have before splitting, so both our training and testing sets have a healthy amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset has 47520 rows and 40 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Our dataset has {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also see what percentage of our dataset each class composes. If the classes are uneven, we will need to perform a stratified split of our data to ensure that our testing dataset has a congruent proportion of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of values in each class is: \n",
      "functional                 0.542971\n",
      "non functional             0.384091\n",
      "functional needs repair    0.072938\n"
     ]
    }
   ],
   "source": [
    "print(\"The proportion of values in each class is: \")\n",
    "print(df.status_group.value_counts(normalize=True).to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classes are imbalanced, especially for 'functional needs repair.' Given that this class comprises only 7% of the data, we will need to be careful about how to split our data. We want as many records of this class in our training set as possible so we can create an accurate model, but we want enough records of this class in our testing set to provide meaningful feedback.\n",
    "\n",
    "Given our near 50,000 records, we should aim to have at least 500 records for the 'functional needs repair' class in our training set. With 500 records, each record in this class will account for 0.2% of the class' prediction accuracy.\n",
    "\n",
    "To find our optimal split percentage:\n",
    "\n",
    "$$ 47520 * split_{pct} * 0.0729 = 500 $$\n",
    "$$ split_{pct} = \\frac{500}{47520 * 0.0729} $$\n",
    "$$ split_{pct} = 0.144 $$\n",
    "\n",
    "Let's make our testing set an even 15% of our total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 40392 rows and our testing set has 7128 rows.\n",
      "\n",
      "The proportion of classes in our training set is: \n",
      "functional                 0.542979\n",
      "non functional             0.384086\n",
      "functional needs repair    0.072935\n",
      "\n",
      "The proportion of classes in our testing set is: \n",
      "functional                 0.542929\n",
      "non functional             0.384119\n",
      "functional needs repair    0.072952\n",
      "\n",
      "The number of 'functional needs repair' records in our testing set is: \n",
      "520\n"
     ]
    }
   ],
   "source": [
    "# split training and testing sets\n",
    "testing_set = df.groupby('status_group', group_keys=False).apply(lambda x: x.sample(frac=0.15))\n",
    "training_set = df.drop(testing_set.index, axis=0)\n",
    "\n",
    "# validate split\n",
    "print(f\"Our training set has {training_set.shape[0]} rows and our testing set has {testing_set.shape[0]} rows.\")\n",
    "print()\n",
    "print(\"The proportion of classes in our training set is: \")\n",
    "print(training_set.status_group.value_counts(normalize=True).to_string())\n",
    "print()\n",
    "print(\"The proportion of classes in our testing set is: \")\n",
    "print(testing_set.status_group.value_counts(normalize=True).to_string())\n",
    "print()\n",
    "print(\"The number of 'functional needs repair' records in our testing set is: \")\n",
    "print(testing_set.status_group.value_counts(ascending=True)[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have separated a testing set to test the accuracy of our model. Now let's start looking at the data in our training set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "We've recieved our data in a CSV format from Kaggle, but we should still ask the question, *where does our data come from?*\n",
    "\n",
    "From the Kaggle competition description:\n",
    "```\n",
    "The data comes from the Taarifa waterpoints dashboard, which aggregates data from the Tanzania Ministry of Water. In their own words:\n",
    "\n",
    "Taarifa is an open-source platform for the crowd-sourced reporting and triaging of infrastructure-related issues. Think of it as a bug tracker for the real world which helps to engage citizens with their local government.\n",
    "```\n",
    "\n",
    "So the data is crowd-sourced, perhaps by the citizens using the pumps, or by volunteers. Either way, there are some questions which remain unanswered by the description:\n",
    "- *Who inputs the data into the dashboard?* Is it the people who use the pumps, volunteers, etc?\n",
    "- *Are datatypes altered between entry and dashboarding?* Has the data changed in the pipeline between user entries and the CSV we received? Is there a potential for information to be lost in this process?\n",
    "- *Who verifies the accuracy of the data?* Does the Ministry of Water verify, or does it rely on crowd-sourcing to correct errors?\n",
    "- *What data is updated, and what data is static?* What data is inputted by dashboard users, and what data remains unaltered since the pump's creation?\n",
    "\n",
    "In leiu of answers to these questions, we will need to make our best educated guesses about the data we're working with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Data Surface Qualities\n",
    "\n",
    "We've already split our data into training and testing sets, and from that, we know that we have 40392 records to train our model, and we're working with 39 features (40, less the dependent variable 'status group') Let's take another look at the breakdown of classes in the records, and this time, we'll also include the record counts for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of classes in our training set is: \n",
      "functional                 0.542979\n",
      "non functional             0.384086\n",
      "functional needs repair    0.072935\n",
      "\n",
      "The record counts of classes in our training set is: \n",
      "functional                 21932\n",
      "non functional             15514\n",
      "functional needs repair     2946\n"
     ]
    }
   ],
   "source": [
    "print(\"The proportion of classes in our training set is: \")\n",
    "print(training_set.status_group.value_counts(normalize=True).to_string())\n",
    "print()\n",
    "print(\"The record counts of classes in our training set is: \")\n",
    "print(training_set.status_group.value_counts().to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the datatype counts, and our datatypes for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The counts for each datatype are: \n",
      "object     31\n",
      "int64       6\n",
      "float64     3\n"
     ]
    }
   ],
   "source": [
    "print(\"The counts for each datatype are: \")\n",
    "print(training_set.dtypes.value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datatypes for each feature are: \n",
      "status_group              object\n",
      "amount_tsh               float64\n",
      "date_recorded             object\n",
      "funder                    object\n",
      "gps_height                 int64\n",
      "installer                 object\n",
      "longitude                float64\n",
      "latitude                 float64\n",
      "wpt_name                  object\n",
      "num_private                int64\n",
      "basin                     object\n",
      "subvillage                object\n",
      "region                    object\n",
      "region_code                int64\n",
      "district_code              int64\n",
      "lga                       object\n",
      "ward                      object\n",
      "population                 int64\n",
      "public_meeting            object\n",
      "recorded_by               object\n",
      "scheme_management         object\n",
      "scheme_name               object\n",
      "permit                    object\n",
      "construction_year          int64\n",
      "extraction_type           object\n",
      "extraction_type_group     object\n",
      "extraction_type_class     object\n",
      "management                object\n",
      "management_group          object\n",
      "payment                   object\n",
      "payment_type              object\n",
      "water_quality             object\n",
      "quality_group             object\n",
      "quantity                  object\n",
      "quantity_group            object\n",
      "source                    object\n",
      "source_type               object\n",
      "source_class              object\n",
      "waterpoint_type           object\n",
      "waterpoint_type_group     object\n"
     ]
    }
   ],
   "source": [
    "print(\"The datatypes for each feature are: \")\n",
    "print(training_set.dtypes.to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'object' datatype can take multiple forms, the most common being string data. However, just because the datatype is 'object' does not mean that all of the data in the column is string data. It is possible for an 'object' datatype to contain multiple datatypes. Our data is likely dirty and needs to be cleaned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Data Quality\n",
    "\n",
    "Given that we know our data might be dirty, this is a good time to see how many null (nonexistent) values there are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a total of 31346 null values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There is a total of {training_set.isnull().sum().sum()} null values in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The counts of null values by feature are: \n",
      "status_group                 0\n",
      "amount_tsh                   0\n",
      "date_recorded                0\n",
      "funder                    2442\n",
      "gps_height                   0\n",
      "installer                 2457\n",
      "longitude                    0\n",
      "latitude                     0\n",
      "wpt_name                     0\n",
      "num_private                  0\n",
      "basin                        0\n",
      "subvillage                 257\n",
      "region                       0\n",
      "region_code                  0\n",
      "district_code                0\n",
      "lga                          0\n",
      "ward                         0\n",
      "population                   0\n",
      "public_meeting            2273\n",
      "recorded_by                  0\n",
      "scheme_management         2650\n",
      "scheme_name              19186\n",
      "permit                    2081\n",
      "construction_year            0\n",
      "extraction_type              0\n",
      "extraction_type_group        0\n",
      "extraction_type_class        0\n",
      "management                   0\n",
      "management_group             0\n",
      "payment                      0\n",
      "payment_type                 0\n",
      "water_quality                0\n",
      "quality_group                0\n",
      "quantity                     0\n",
      "quantity_group               0\n",
      "source                       0\n",
      "source_type                  0\n",
      "source_class                 0\n",
      "waterpoint_type              0\n",
      "waterpoint_type_group        0\n"
     ]
    }
   ],
   "source": [
    "print(\"The counts of null values by feature are: \")\n",
    "print(training_set.isnull().sum().to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have plenty of null values in our data, the most egregious omission being in the 'scheme_name' feature. We will need to determine the importance of this column, as we will likely drop it in the cleaning process because there is so much data missing.\n",
    "\n",
    "It is important to note that these are only the *explicit* null values, or values explicitly left blank. We will also need to be vigilant in finding *implicit* null values - values which appear as valid in our dataset, but are intended as nulls (such as an 'x' in a field where a string name should be). To identify these possible implicit nulls, we will need to better understand the features we're working with so we can determine which values are valid."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Features\n",
    "\n",
    "We have 39 features in our dataset. Let's see what forms they take, and do the research necessary to understand them. This will enable us to achieve better results in the remaining analysis, cleaning, and modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a16043d5189dac7523fbdbef2db282625aa16f53015e4658daeaf20222b94251"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
